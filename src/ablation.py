import os
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from transformers import BertTokenizer
from tqdm import tqdm

from dataset import MultiModalDataset
from model import MultimodalSentimentModel

# --- é…ç½® ---
BATCH_SIZE = 16
MAX_LEN = 128
DATA_DIR = "../data"
# ä½¿ç”¨ä¹‹å‰åˆ’åˆ†å¥½çš„éªŒè¯é›†
VAL_FILE = "../data/val_split.csv" 
MODEL_PATH = "../output/best_model.pth"

if torch.backends.mps.is_available():
    DEVICE = torch.device("mps")
elif torch.cuda.is_available():
    DEVICE = torch.device("cuda")
else:
    DEVICE = torch.device("cpu")

def evaluate(model, loader, tokenizer, mode='multimodal'):
    """
    mode: 
      - 'multimodal': æ­£å¸¸é¢„æµ‹
      - 'text_only': æŠŠå›¾ç‰‡å…¨ç½®ä¸º0
      - 'image_only': æŠŠæ–‡æœ¬å…¨ç½®ä¸º0
    """
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for texts, images, labels, guids in tqdm(loader, desc=f"Testing {mode}"):
            images = images.to(DEVICE)
            labels = labels.to(DEVICE)
            
            encoded_text = tokenizer(
                list(texts),
                padding=True,
                truncation=True,
                max_length=MAX_LEN,
                return_tensors='pt'
            )
            input_ids = encoded_text['input_ids'].to(DEVICE)
            attention_mask = encoded_text['attention_mask'].to(DEVICE)
            
            # === æ ¸å¿ƒï¼šæ¶ˆèé€»è¾‘ ===
            if mode == 'text_only':
                # å°†å›¾ç‰‡å…¨è®¾ä¸º 0 (é»‘è‰²)
                images = torch.zeros_like(images)
            elif mode == 'image_only':
                # å°†æ–‡æœ¬ Mask å…¨è®¾ä¸º 0 (è®©æ¨¡å‹ä»¥ä¸ºæ²¡æœ‰ä»»ä½•å­—)
                attention_mask = torch.zeros_like(attention_mask)
                input_ids = torch.zeros_like(input_ids)
            # ===================
            
            outputs = model(input_ids, attention_mask, images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
    acc = 100 * correct / total
    return acc

def run_ablation():
    print("ğŸ”¬ å¼€å§‹æ¶ˆèå®éªŒ...")
    
    # å‡†å¤‡æ•°æ®å’Œæ¨¡å‹
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    val_dataset = MultiModalDataset(DATA_DIR, VAL_FILE, transform=transform, mode='train')
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = MultimodalSentimentModel(num_classes=3).to(DEVICE)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    
    # 1. æµ‹è¯•å¤šæ¨¡æ€ (å®Œæ•´)
    acc_multi = evaluate(model, val_loader, tokenizer, mode='multimodal')
    print(f" å¤šæ¨¡æ€ (Multimodal) å‡†ç¡®ç‡: {acc_multi:.2f}%")
    
    # 2. æµ‹è¯•ä»…æ–‡æœ¬
    acc_text = evaluate(model, val_loader, tokenizer, mode='text_only')
    print(f" ä»…æ–‡æœ¬ (Text Only) å‡†ç¡®ç‡: {acc_text:.2f}%")
    
    # 3. æµ‹è¯•ä»…å›¾åƒ
    acc_img = evaluate(model, val_loader, tokenizer, mode='image_only')
    print(f" ä»…å›¾åƒ (Image Only) å‡†ç¡®ç‡: {acc_img:.2f}%")
    
    print("\n--- å®éªŒç»“è®ºå»ºè®® ---")
    print(f"ä½ çš„æŠ¥å‘Šé‡Œåº”è¯¥ç”»ä¸ªè¡¨ï¼Œå¡«å…¥è¿™ä¸‰ä¸ªæ•°å­—ã€‚")
    print("ç†è®ºä¸Šï¼Œå¤šæ¨¡æ€åº”è¯¥æœ€é«˜ï¼Œå…¶æ¬¡æ˜¯æ–‡æœ¬ï¼Œå›¾ç‰‡é€šå¸¸æœ€ä½ã€‚")

if __name__ == "__main__":
    run_ablation()
