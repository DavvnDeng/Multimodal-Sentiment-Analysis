# Multimodal Sentiment Analysis (Experiment 5)

> åä¸œå¸ˆèŒƒå¤§å­¦ æ•°æ®ç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢ æ·±åº¦å­¦ä¹ å®éªŒäº”ï¼šå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ç±»

This repository contains the implementation of a Multimodal Sentiment Analysis model based on **Late Fusion** strategy. It combines **BERT** (for text) and **ResNet-50** (for image) to classify social media posts into three sentiment categories: `Positive`, `Neutral`, and `Negative`.

## ğŸ“Œ Project Overview

*   **Task**: Multimodal Sentiment Classification (3 classes).
*   **Model Architecture**: BERT + ResNet50 + Concatenation + MLP.
*   **Key Features**:
    *   **Robust Data Loading**: Fixed GUID parsing bugs for inconsistent CSV formats.
    *   **Data Augmentation**: Random Horizontal Flip & Rotation implemented for training.
    *   **Evaluation**: Comprehensive ablation studies and bad case analysis.
*   **Performance**: Achieved **72.12%** accuracy on the validation set.

## ğŸ“‚ File Structure

```text
Multimodal-Sentiment-Analysis/
â”œâ”€â”€ data/                   # Data folder (Excluded from git)
â”‚   â”œâ”€â”€ train.txt           # Training labels
â”‚   â”œâ”€â”€ test_without_label.txt
â”‚   â”œâ”€â”€ *.jpg               # Image files
â”‚   â””â”€â”€ *.txt               # Text files
â”œâ”€â”€ output/                 # Model checkpoints and logs
â”‚   â”œâ”€â”€ best_model.pth      # Best trained weights
â”‚   â””â”€â”€ bad_cases.csv       # Analysis of error samples
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ dataset.py          # Data loader with robust parsing logic
â”‚   â”œâ”€â”€ model.py            # Model architecture (BERT + ResNet)
â”‚   â”œâ”€â”€ train.py            # Training loop with Data Augmentation
â”‚   â”œâ”€â”€ predict.py          # Inference script for test set
â”‚   â”œâ”€â”€ ablation.py         # Ablation study script
â”‚   â””â”€â”€ analyze_bad_cases.py # Error analysis tool
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ test_result.txt         # Final submission file
â””â”€â”€ README.md               # Project documentation
```

## ğŸ› ï¸ Environment Requirements

To set up the environment, run:

```
pip install -r requirements.txt
```

**Main Dependencies:**
*   Python 3.8+
*   PyTorch
*   Torchvision
*   Transformers (HuggingFace)
*   Pandas, Pillow, Scikit-learn, Tqdm

## ğŸš€ Execution Flow

### 1. Data Preparation
Please unzip the provided dataset `å®éªŒäº”æ•°æ®.zip` and place all files into the `data/` directory.

### 2. Training
Train the model from scratch. This script includes data augmentation and automatically saves the best model to `output/best_model.pth`.

```
cd src
python train.py
```

### 3. Inference (Prediction)
Generate predictions for `test_without_label.txt`. The result will be saved as `test_result.txt` in the root directory.

```
python predict.py
```

### 4. Ablation Study
Evaluate the contribution of each modality (Text-Only vs Image-Only vs Multimodal).

```
python ablation.py
```

### 5. Error Analysis
Identify and save misclassified samples from the validation set to `output/bad_cases.csv` for analysis.

```
python analyze_bad_cases.py
```

## ğŸ“Š Results

### Validation Performance
| Model Setting | Text Input | Image Input | Accuracy |
| :--- | :---: | :---: | :---: |
| Image Only | âŒ | âœ… | 60.38% |
| Text Only | âœ… | âŒ | 66.25% |
| **Multimodal (Ours)** | **âœ…** | **âœ…** | **72.12%** |

## ğŸ“ References

This project is implemented based on the following papers and repositories:

**Papers:**
*   **BERT**: Devlin, J., et al. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." (NAACL 2019).
*   **ResNet**: He, K., et al. "Deep Residual Learning for Image Recognition." (CVPR 2016).

**Repositories:**
*   [HuggingFace Transformers](https://github.com/huggingface/transformers)
*   [PyTorch Vision](https://github.com/pytorch/vision)
*   [GloGNN (Readme Style Reference)](https://github.com/RecklessRonan/GloGNN)
```
